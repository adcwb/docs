### 简介

在爬虫中为什么需要使用数据解析？

- 帮助我们可以实现聚焦爬虫

- 聚焦爬虫实现流程：

    - 1.指定url
    - 2.发起请求
    - 3.获取响应数据
    - 4.数据解析
    - 5.持久化存储

    

数据解析通用原理

- HTML作用？
    - HTML主要的作用就是用来展示数据
- 2.我们要解析的数据其实都是存储在
    - HTML的标签之中
    - 标签的属性
- 通用原理：
    - 定位标签
    - 获取标签中的数据或者标签的属性值

### 数据解析的常用方法



#### 正则

```Python
import re
import os


dirName = 'ImgLibs'
if not os.path.exists(dirName):
    os.mkdir(dirName)

url = 'https://sc.chinaz.com/tupian/renwusuxie.html'
page_text = requests.get(url=url,headers=headers).text
#数据解析:解析出图片地址
ex = '<div class="box picblock col3".*?<img src2="(.*?)" alt.*?</div>'
img_src = re.findall(ex,page_text,re.S)
for src in img_src:
    src = 'https:'+src
    img_data = requests.get(url=src,headers=headers).content
    img_name = src.split('/')[-1]
    img_path = dirName+'/'+img_name
    with open(img_path,'wb') as fp:
        fp.write(img_data)
    print(img_name,'下载完毕!')
```



#### bs4数据解析

```Python
环境安装：
	pip install bs4
    pip install lxml
    
bs4解析原理：
    实例化一个BeautifulSoup的对象，且将被解析的页面数据加载到该对象中
    调用BeautifulSoup对象中相关的方法和属性进行标签定位和相关数据的提取
    
BeautifulSoup的对象实例化方式：
    方式1：BeautifulSoup(fp,'lxml')
    可以将本地存储的html文件中的数据进行数据解析
    方式2：BeautifulSoup(page_text,'lxml')
    可以将互联网上请求到的页面数据进行数据解析
    
    
标签定位：
	soup.tagName:可以定位到第一次出现的该标签
        
属性定位：
    soup.find('tagName',attrName='attrValue')
    	注意：find只可以定位到第一次出现的该标签
    soup.find_all('tagName',attrName='attrValue')
    	注意：find_all可以定位到符合条件所有的标签，返回值一定是列表
        
选择器定位：
	select('选择器')
	id，类，层级选择器
	层级选择器：
        大于号：表示一个层级
        空格：表示多个层级
文本提取
	tag.string:获取标签直系的文本内容
    tag.text:获取标签下所有的文本内容
        
属性提取
	tag['attrName']
```



##### 案例：爬取诗词名句网

```Python
需求：
	将三国演义整片内容爬取和保存
    
url：
	http://www.shicimingju.com/book/sanguoyanyi.html
```

```Python
import requests
from bs4 import BeautifulSoup
headers = {
    'User-Agent':'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'
}

fp = open('sanguo.txt','w',encoding='utf-8')
url = 'http://www.shicimingju.com/book/sanguoyanyi.html'
page_text = requests.get(url=url,headers=headers).text
#数据解析：章节标题+详情页的url
soup = BeautifulSoup(page_text,'lxml')
a_list = soup.select('.book-mulu > ul > li > a')
for a in a_list:
    title = a.string
    detail_url = 'http://www.shicimingju.com'+a['href']
    #对详情页的url发起请求获取详情页的页面源码数据
    page_text_detail = requests.get(url=detail_url,headers=headers).text
    #数据解析：章节内容
    detail_soup = BeautifulSoup(page_text_detail,'lxml')
    content = detail_soup.find('div',class_='chapter_content').text
    fp.write(title+':'+content)
    print(title,'下载保存成功！！！')
fp.close()

```



#### xpath解析

```Python
环境安装：
	pip install lxml
    
xpath解析的原理：
    - html标签是基于树状的结构存在的
    	- 1.实例化一个etree的对象，且将被解析的数据加载到该对象
    	- 2.调用etree对象中的xpath方法结合着不同形式的xpath表达式进行标签定位和数据提取
        
etree对象的实例化两种方式：
    方式1：
    	etree.parse('filename')
    方式2：
    	etree.HTML(page_text)
        
        
标签定位
    最左侧的/:一定要从根标签开始进行指定标签的定位
    非最左侧的/:表示一个层级
    非最左侧的//:表示多个层级
    最左侧的//:可以从任意位置定位指定标签（常用）
        
属性定位：
	//tagName[@attrName="attrValue"]
    
索引定位：
	//tagName[index],注意索引是从1开始
    
模糊匹配：
	//div[contains(@class, "ng")]：定位到class属性值中包含ng的div标签
	//div[starts-with(@class, "ta")]：定位到class属性值是以ta开头的标签

取文本
	/text():可以将标签中直系的文本内容取出，且返回的列表元素只有一个
	//text()：可以将标签中所有的文本内容取出，且返回的列表元素为多个

取属性
	/@attrName
```



##### 案例： 爬取糗事百科的段子内容

```Python
url_model = 'https://www.qiushibaike.com/text/page/%d/'
for page in range(1,14):
    url = format(url_model%page)
    page_text = requests.get(url=url,headers=headers).text
    #数据解析：段子作者+内容
    tree = etree.HTML(page_text)
    #定位标签
    div_list = tree.xpath('//div[@class="col1 old-style-col1"]/div')
    for div in div_list:
        #局部的数据解析
            #每次循环使用的div表示就是页面中定位到的指定div标签（包含了要解析的内容）
        #注意：./就是xpath方法的调用者
        author = div.xpath('./div[1]/a[2]/h2/text()')[0] #想要从div表示的指定标签数据中进行指定数据的解析
        content = div.xpath('./a[1]/div/span//text()')
        content = ''.join(content)
        print(author,content)
```



#### 案例： 爬取段子网的内容

```Python

#将第一页的内容进行爬取
url = 'https://duanziwang.com/'
response = requests.get(url,headers=headers)
response.encoding = 'utf-8' #修改响应对象的编码格式来处理乱码问题
page_text = response.text
tree = etree.HTML(page_text)

#全局数据解析
article_list = tree.xpath('/html/body/section/div/div/main/article')
for article in article_list:
    #局部数据解析
    #./表示局部解析中xapth的调用者
    title = article.xpath('./div[1]/h1/a/text()')[0]
    content = article.xpath('./div[2]/pre/code/text()')[0]
    print(title,content)
    
```

```Python
爬取多页
#定义个通用的url模板:生成其他页码的url
url = 'https://duanziwang.com/page/%d/index.html'
for page in range(1,6):
    if page == 1:
        new_url = 'https://duanziwang.com/'
    else:
        new_url = format(url%page)
    print('正在爬取第%d页的数据'%page)
    response = requests.get(new_url,headers=headers)
    response.encoding = 'utf-8' #修改响应对象的编码格式来处理乱码问题
    page_text = response.text
    tree = etree.HTML(page_text)
    #全局数据解析
    article_list = tree.xpath('/html/body/section/div/div/main/article')
    for article in article_list:
        #局部数据解析
        #./表示局部解析中xapth的调用者
        title = article.xpath('./div[1]/h1/a/text()')[0]
        content = article.xpath('./div[2]/pre/code/text()')[0]
        print(title,content)

```



#### 拓展：百度AI语音合成

​		可以将文字数据进行语音合成

​		使用百度AI进行语音合成

```PYTHON 
from aip import AipSpeech

""" 你的 APPID AK SK """
APP_ID = '23****89'
API_KEY = 'eTvorFk6cfI12Z*********'
SECRET_KEY = '92gvtMbfTzuR3n0e6ZVSibqH*******'

client = AipSpeech(APP_ID, API_KEY, SECRET_KEY)

text = '先生,电梯仍在正常运行,只不过您进的那是电话间'
result  = client.synthesis(text, 'zh', 1, {
    'vol': 5,
    'per':4
})

# 识别正确返回语音二进制 错误则返回dict 参照下面错误码
if not isinstance(result, dict):
    with open('aaa.mp3', 'wb') as f:
        f.write(result)
```





#### 案例： 爬取彼岸图库图片

```朋友thin
import requests
import bs4
import os 
import re
import time
from lxml import etree

url = 'http://pic.netbian.com/4kmeinv/index_%d.html'
first_url = "http://pic.netbian.com/"
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36',
    'Referer': 'http://pic.netbian.com/4kmeinv/'
}
img_src = []
sess = requests.Session()
sess.get(url=first_url,headers=headers)

if not os.path.exists("images"):
    os.mkdir("images")

for page in range(1,172):
    if page == 1:
        new_url = 'http://pic.netbian.com/4kmeinv/'
    else:
        new_url = 'http://pic.netbian.com/4kmeinv/index_%d.html' %(page)

    response = sess.get(new_url, headers=headers)
    response.encoding = 'gbk'
    page_text =response.text
    
    tree = etree.HTML(page_text)
       
    li_list = tree.xpath('//*[@id="main"]/div[3]/ul/li//@href')
    for img_url in li_list:

        img_src_url = "http://pic.netbian.com" + img_url
        img_src.append(img_src_url)

for img in img_src:
    response = sess.get(img, headers=headers)
    response.encoding = 'gbk'
    page_text =response.text
    tree = etree.HTML(page_text)

    
    li_list = tree.xpath('//*[@id="img"]/img/@src')

    images = "http://pic.netbian.com" + li_list[0]
    img_info = sess.get(images, headers=headers).content
    file_name = images.rsplit('/',1)[-1]
    with open('images/'+file_name, 'wb') as fp:
        time.sleep(0.1)
        fp.write(img_info)
    print(file_name, '下载完成')
    

    
    
    
    
    
```

